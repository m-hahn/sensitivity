# Code

## Sensitivity Bounds for ML Models (Section 3)

* randomly initialized LSTMs

* LSTM learnability 

* Transformer learnability (done after the paper was finished):

## Sensitivity of NLP Tasks (Section 4)

* generating alternatives using XLNet

* generating alternatives using u-PMLM: see https://github.com/m-hahn/PMLM

* creating RoBERTa predictions: see https://github.com/m-hahn/fairseq

* creating BoE and LSTM predictions

* creating GPT-2 predictions for Syntax tasks

* Sensitivity and length

* performance of BoE, LSTN, RoBERTa

* per-input analysis

* Role of task model

* Evaluating lower-bound approximation

## Human Studies (Section 5)

* Experiment 1
** Code:
** Link to online experiment:
** Results:

* Experiment 2
** Code:
** Link to online experiment:
** Results:
